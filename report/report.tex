\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}	% For figure environment
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}


\begin{document}
\title{Brilliant Title}

\author{
  Ben Hahn, Kevin Klein, Lorenz Kuhn\\
  Department of Computer Science, ETH Zurich, Switzerland
}

\maketitle

\begin{abstract}
  A critical part of scientific discovery is the
  communication of research findings to peers or the general public.
  Mastery of the process of scientific communication improves the
  visibility and impact of research. While this guide is a necessary
  tool for learning how to write in a manner suitable for publication
  at a scientific venue, it is by no means sufficient, on its own, to
  make its reader an accomplished writer. We also describe the rules
  for submission in the computational intelligence laboratory.
  This guide should be a
  starting point for further development of writing skills.
\end{abstract}

\section{Introduction}

Many online businesses face the difficult yet crucial challenge of finding the most relevant products out of a enormous set of options for each of their users. Providing better recommendations, such as suggesting a movie on Netflix or a product on Amazon that a user might like, has been found to be linked to increased sales, an increase in consumer surplus and competitive advantages \cite{hinz2010impact}.
Given the gigantic set of options, more than 570 million products are currently available on Amazon.com \cite{scrap2018}, a wide variance in preferences between different users and relatively little knowledge about individual users, this is a challenging task. Collaborative filtering \cite{sarwar2001item} is an approach to this problem in which users' preferences are modelled based on their past interactions with the system. These methods are based on the fundamental assumption that similarities between the users in terms of their past preferences can be exploited to make new recommendations. Whereas many applications in industry are based on implicit feedback, such as the number of times a user has clicked on or viewed a certain product, we work with explicit feedback, that is ratings of movies on an integer scale. 

    
Traditionally, k-nearest neighbour approaches have been applied to this problem. That is, the ratings of the most similar users or items are exploited to infer the rating for a given user and item \cite{sarwar2001item}.

Recently, the most popular approach has been to represent users and items  as vectors in a shared, low-dimensional latent space intended to capture the hidden factors influencing the users' preferences. These embedding may be obtained and then combined in a linear fashion using matrix factorization techniques \cite{koren2009matrix}, or in a non-linear way using Neural Networks
\cite{he2017neural}.


((Summarize work on ensemble method we use.))



We draw on and improve previous work by proposing an approach which applies a state-of-the-art ensemble method to recommendations produced by both sophisticated matrix factorization techniques as well as neural network models.

Our main contributions are thus the following: 
\begin{enumerate}
    \item ((Kevin's bit on MF techniques.))
    \item Following the idea of pretraining neural networks, we use existing embeddings as input for our neural network. We obtain embeddings using a number of different methods which are then used as input of a feed-forward neural network to model nonlinear interactions between the users and the items. 
    \item ((Ben's bit on ensemble methods))
\end{enumerate}



\section{Models and Methods}
\label{sec:methods}

\subsection{Task}

The specific task we address is the following.
We are given a set of observed ratings $\mathcal{R}$. For $(u, i, r_{ui}) \in \mathcal{R}$, $u \in [1\dots10000]$ is a user id, $i \in [1\dots1000]$  is an item id and $r_{ui} \in [1\dots5]$ the rating of the user for this item. $\mathcal{R}$ contains 11.8\% of all possible ratings. 

Furthermore, we are given a test set $\mathcal{T}$, where $(u,i) \in \mathcal{T}$ are unobserved tuples of user and item ids for which we want to infer the ratings

The goal of this task is to minimize the Root Mean Squared Error (RMSE) given by:
\begin{equation}
RMSE = \sqrt{\frac{1}{|\mathcal{T}|}\sum_{(u, i) \in \mathcal{T}} (r_{u,i} -\widehat{r}_{u,i})^2}    
\end{equation}

where $\mathcal{T}$ is a test set of ratings in which $r_{u, i}$ is the actual rating of user $u$ for movie $i$ and $\widehat{r}_{u,i}$ the corresponding prediction. 

((@Ben: Please extend this with some of the data analysis results that you got.))

\subsection{Preprocessing}

* Why we need to do this in the first place
* What approaches we follow:
    by avg, by bias, "novel"
    
Many matrix factorization techniques, such as SVD, require full matrices, that is ratings for all user and movie pairs, as initial input. As we only observe a small subset of ratings, we face the problem of inferring the unobserved ratings to generate a valid input for our main methods. To this end, we use a number of different approaches.

\subsubsection{Naive Averages}
One straightforward way of inferring an unobserved rating $r_{ij}$ is to set unobserved ratings to the mean of the observations in row $i$ or column $j$ respectively. This captures the intuition that a user's rating for a film is going to be close to the user's other ratings or the films other ratings respectively.

\subsubsection{Biases}
A more sophisticated idea is to compute how much the average rating for a given user or a given film deviates from the total average of all observed ratings: $user\_bias_i = \frac{1}{N_i} \sum_{j : r_{ij} \in R} r_{ij} - \frac{1}{N} \sum_{r_{ij} \in R} r_{i,j}$ , where $N_i$ is the number of observations for user $i$, $N$ the total number of observations and $R$ the set of observed ratings. Analogously, $movie\_bias_j$ can be computed for all movies. 

Given these biases, we then compute the initialization for an unobserved rating $r_{ij}$ as $r_{ij} = \frac{1}{N} \sum_{r_{ij} \in R} r_{i,j} + user\_bias_i + movie\_bias_j$.

\subsubsection{"Novel init"}
((Decide whether and in what form we want to include this based on results we get.))

\subsection{Core}
\subsubsection{Matrix Factorization}
\subsubsection{Neural Network}
In addition to the models based on matrix factorization techniques, our system employs a number of predictors based on neural networks. Firstly, we follow the approach proposed in Neural Collaborative Filtering \cite{he2017neural}, where embedding vectors for users and items are learned from random initialisations which are then fed into a feedforward neural network. The model parameters are then trained using standard backpropagation methods to minimise the squared loss on the observed ratings. 
Secondly, we propose the use of existing embedding vectors obtained through other methods as input to a neural network regressor. Rather than having to learn the embedding vectors, this approach exploits precomputed embeddings while being able to model complex, nonlinear interactions  between the users and the items. 
a number of neural network models are trained to predict ratings given user and item embeddings obtained through a number of different dimensionality reduction techniques: Locally Linear Embeddings \cite{roweis2000nonlinear}, Non-Negative Matrix Factorisation \cite{cichocki2009fast}, standard SVD and iterated SVD as presented above.

In particular the neural network model used is the following:
\begin{equation}
\begin{aligned}
    \bf{z_1} &= \phi_1(\bf{e_i}, \bf{e_u}) = [\bf{e_i}, \bf{e_u}]\\
    \bf{z_2} &= a_2(\bf{W_2}^T \bf{z_1} + b_2) \\
    &\dots \\
    \bf{z_L} &= a_{L - 1 }(\bf{W_{L}}^T \bf{z_{L - 1}} + b_L) \\
    \hat y_{ui} &= \sigma(\bf{h}^T \bf{z_L})
\end{aligned}
\end{equation}

where $a_i,\bf{W_i}, \bf{b_i}$, denote the i-th layer's activation function, weights and bias respectively. With $\bf{e_i}$ and $\bf{e_u}$ we denote the embedding vectors for the users and items respectively. We use the rectifier (ReLU) activation function, which has been found to frequently yield superior results as compared to the traditionally used sigmoid and tanh functions \cite{glorot2011deep}.


\begin{table}[ht]
\centering
\caption{Validation Results of Neural Network Models}
\label{table:neural_net_models}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
     & Random Init & SVD & Iterated SVD & LLE & NMF \\ \hline
RMSE & 1.117           & 0.993   & 0.997           & 1.08   & 0.995   \\ \hline
\end{tabular}
\end{table}

Using grid search in the, we find hyperparameters yielding the best results on the validation set (see table \ref{table:neural_net_models}). 
With regards to the dimensionality of the embedding space, we find that $\mathbf{e_i}, \mathbf{e_u} \in  \mathbb{R}^{20}$ leads to the best results. We find that two hidden layers with 10 and 5 nodes respectively yield the best RMSE. For the randomly initialised embeddings, an additional hidden layer is added and the layer widths are increased.((Into how much detail do we need to go here?))




\subsubsection{Ensemble Methods}

\subsection{Postprocessing}
As a final step, we smooth the predictions based on the user's  k-Nearest-Neighbors in the embedding space obtained through SVD. Following Bell et al. \cite{bell2007improved}

\subsection{temp}
\subsubsection{Iterated SVD}
\begin{algorithmic}
	\STATE $R$: Ratings matrix with holes, $k$ fixed rank
	\STATE $M \leftarrow R$
	\STATE Impute $M$ by initialization
    \FOR {$i \in \{1 \dots n_{epochs}\}$} 
    	\STATE ($U, \Sigma, D) \leftarrow SVD(M)$
    	\STATE $U_{(k)} \leftarrow U[:, 1:k]$
    	\STATE $\Sigma_{(k)} \leftarrow \Sigma[1:k, 1:k]$
    	\STATE $D_{(k)} \leftarrow D[:, 1:k]$
    	\STATE $M \leftarrow R$
    	\STATE Impute $M$ by $U_{(k)} \Sigma_{(k)} D_{(k)}^T$
    \ENDFOR
    \RETURN $M$
\end{algorithmic}


\section{Results}
\label{sec:results}


\section{Discussion}
\label{sec:discussion}

\section{Summary}


\bibliographystyle{IEEEtran}
\bibliography{report}
\end{document}

