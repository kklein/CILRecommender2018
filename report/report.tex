\documentclass[10pt,conference,compsocconf]{IEEEtran}

%\usepackage{times}
%\usepackage{balance}
\usepackage{url}
\usepackage{graphicx}	% For figure environment
\usepackage{algorithmic}
\usepackage{amsmath}


\begin{document}
\title{Brilliant Title}

\author{
  Ben Hahn, Kevin Klein, Lorenz Kuhn\\
  Department of Computer Science, ETH Zurich, Switzerland
}

\maketitle

\begin{abstract}
  A critical part of scientific discovery is the
  communication of research findings to peers or the general public.
  Mastery of the process of scientific communication improves the
  visibility and impact of research. While this guide is a necessary
  tool for learning how to write in a manner suitable for publication
  at a scientific venue, it is by no means sufficient, on its own, to
  make its reader an accomplished writer. We also describe the rules
  for submission in the computational intelligence laboratory.
  This guide should be a
  starting point for further development of writing skills.
\end{abstract}

\section{Introduction}

Many online businesses face the difficult yet crucial challenge of finding the most relevant products out of a enormous set of options for each of their users. Providing better recommendations, such as suggesting a movie on Netflix or a product on Amazon that a user might like, has been found to be linked to increased sales, an increase in consumer surplus and competitive advantages \cite{hinz2010impact}.
Given the gigantic set of options, more than 570 million products are currently available on Amazon.com \cite{scrap2018}, a wide variance in preferences between different users and relatively little knowledge about individual users, this is a challenging task. Collaborative filtering \cite{sarwar2001item} is an approach to this problem in which users' preferences are modelled based on their past interactions with the system. These methods are based on the fundamental assumption that similarities between the users in terms of their past preferences can be exploited to make new recommendations. Whereas many applications in industry are based on implicit feedback, such as the number of times a user has clicked on or viewed a certain product, we work with explicit feedback, that is ratings of movies on an integer scale. 
One popular approach in collaborative filtering has been to use matrix factorization techniques. Here, users and items are represented as vectors in a shared, low-dimensional latent space intended to capture the hidden factors influencing the users' preferences \cite{koren2009matrix}. By calculating the inner product of a user embedding and an item embedding one can obtain the estimated preference of the user for the item. ((Summarize the matrix factorization work, that we use. Regularized svd, sgd, simon funk.))

Recently, He et al.\ \cite{he2017neural} have employed deep neural networks to model the interaction between users and items, arguing that the linear interactions provided by matrix factorization techniques are overly restrictive.
For the final result, recommendations produced by the neural network are combined with recommendations obtained from a simple matrix factorization component.

((Summarize work on ensemble method we use.))



We draw on and improve previous work by proposing an approach which applies a state-of-the-art ensemble method to recommendations produced by both sophisticated matrix factorization techniques as well as neural network models.

Our main contributions are thus the following: 
\begin{enumerate}
    \item ((Kevin's bit on MF techniques.))
    \item Following the idea of pretraining neural networks, we use existing embeddings as input for our neural network We obtain embeddings using a number of different methods which are then used as input of a feed-forward neural network to model nonlinear interactions between the users and the items. 
    \item ((Ben's bit on ensemble methods))
\end{enumerate}



\section{Models and Methods}
\label{sec:methods}

\subsection{Task}

The specific task we address is the following. Given a set of ratings $\mathcal{R}$, where $r_{ij} \in \mathcal{R}$ is the rating of user $i$ for movie $j$, $r_{ij} \in [1 \dots 5]$, $i \in [1 \dots 10000]$ and $j \in [1 \dots 1000]$ ((Does this formulation mean that all i,j in the ranges are in R?)). Out of the $10000 \times 1000$ possible ratings, we observe around 10\% ((Look this up)). The goal is to predict $\hat\mathcal{R}$, where $\hat r_{i'j'} \in \hat\mathcal{R} \rightarrow \hat r_{ij} \not\in \mathcal{R}$. ((@Ben: Please extend this with some of the data analysis results that you got.))
\subsection{Preprocessing}

((Initialization methods))

\subsection{Core}
\subsubsection{Matrix Factorization}
\subsubsection{Neural Network}
In addition to the models based on matrix factorization techniques, our system employs a number of predictors based on neural networks. Firstly, we follow the approach proposed in Neural Collaborative Filtering \cite{he2017neural}, where embedding vectors for users and items are learned from random initialisations which are then fed into a feedforward neural network. The model parameters are then trained using standard backpropagation methods to minimise the squared loss on the observed ratings. 
Secondly, we propose the use of existing embedding vectors obtained through other methods as input to a neural network regressor. Rather than having to learn the embedding vectors, this approach exploits precomputed embeddings while being able to model complex, nonlinear interactions  between the users and the items. 
a number of neural network models are trained to predict ratings given user and item embeddings obtained through a number of different dimensionality reduction techniques: Locally Linear Embeddings \cite{roweis2000nonlinear}, Non-Negative Matrix Factorisation \cite{cichocki2009fast}, standard SVD and iterated SVD as presented above.

In particular the neural network model used is the following:
\begin{equation}
\begin{aligned}
    \bf{z_1} &= \phi_1(\bf{e_i}, \bf{e_u}) = [\bf{e_i}, \bf{e_u}]\\
    \bf{z_2} &= a_2(\bf{W_2}^T \bf{z_1} + b_2) \\
    &\dots \\
    \bf{z_L} &= a_{L - 1 }(\bf{W_{L}}^T \bf{z_{L - 1}} + b_L) \\
    \hat y_{ui} &= \sigma(\bf{h}^T \bf{z_L})
\end{aligned}
\end{equation}

where $a_i,\bf{W_i}, \bf{b_i}$, denote the i-th layer's activation function, weights and bias respectively. With $\bf{e_i}$ and $\bf{e_u}$ we denote the embedding vectors for the users and items respectively. We use the rectifier (ReLU) activation function, which has been found to frequently yield superior results as compared to the traditionally used sigmoid and tanh functions \cite{glorot2011deep}.


\begin{table}[]
\centering
\caption{Validation Results of Neural Network Models}
\label{my-label}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
     & Random Init & SVD & Iterated SVD & LLE & NMF \\ \hline
RMSE & 1.117           & 1.006   & 1.005           & 1.08   & 1.0   \\ \hline
\end{tabular}
\end{table}




((Write up results))

\subsubsection{Ensemble Methods}

\subsection{Postprocessing}
((KNN-smoothing))

\subsection{temp}
\subsubsection{Iterated SVD}
\begin{algorithmic}
	\STATE $R$: Ratings matrix with holes, $k$ fixed rank
	\STATE $M \leftarrow R$
	\STATE Impute $M$ by initialization
    \FOR {$i \in \{1 \dots n_{epochs}\}$} 
    	\STATE ($U, \Sigma, D) \leftarrow SVD(M)$
    	\STATE $U_{(k)} \leftarrow U[:, 1:k]$
    	\STATE $\Sigma_{(k)} \leftarrow \Sigma[1:k, 1:k]$
    	\STATE $D_{(k)} \leftarrow D[:, 1:k]$
    	\STATE $M \leftarrow R$
    	\STATE Impute $M$ by $U_{(k)} \Sigma_{(k)} D_{(k)}^T$
    \ENDFOR
    \RETURN $M$
\end{algorithmic}


\section{Results}
\label{sec:results}


\section{Discussion}
\label{sec:discussion}

\section{Summary}


\bibliographystyle{IEEEtran}
\bibliography{report}
\end{document}

